"""
Performance Metrics Visualization Tool

Visualizes performance metrics from federated learning experiments including:
- Round durations
- Network traffic
- Aggregation times
- Evaluation times

Usage:
    python visualize_metrics.py
    
Requirements:
    - matplotlib
    - performance_metrics.json (generated by server.py)
"""

import json
import matplotlib.pyplot as plt
import os

def load_metrics(filename='performance_metrics.json'):
    """Load performance metrics from JSON file"""
    if not os.path.exists(filename):
        print(f"Error: {filename} not found!")
        print("Run the server first to generate performance metrics.")
        return None
    
    with open(filename, 'r') as f:
        return json.load(f)

def plot_metrics(metrics):
    """Create comprehensive performance metrics visualization"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    rounds = metrics['rounds']
    
    # 1. Round durations
    ax1.plot(rounds, metrics['round_durations'], 'b-o', linewidth=2, markersize=8)
    ax1.set_title('Round Duration Over Time', fontsize=13, fontweight='bold')
    ax1.set_xlabel('Round', fontsize=11)
    ax1.set_ylabel('Time (seconds)', fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # Add average line
    avg_duration = sum(metrics['round_durations']) / len(metrics['round_durations'])
    ax1.axhline(y=avg_duration, color='r', linestyle='--', alpha=0.5, 
                label=f'Average: {avg_duration:.2f}s')
    ax1.legend()
    
    # 2. Network overhead
    network_mb = [b/1024/1024 for b in metrics['network_overhead']]
    ax2.bar(rounds, network_mb, color='green', alpha=0.7, edgecolor='black')
    ax2.set_title('Network Traffic Per Round', fontsize=13, fontweight='bold')
    ax2.set_xlabel('Round', fontsize=11)
    ax2.set_ylabel('Data (MB)', fontsize=11)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # Add value labels
    for r, mb in zip(rounds, network_mb):
        ax2.text(r, mb + max(network_mb)*0.02, f'{mb:.2f}', 
                ha='center', va='bottom', fontsize=9)
    
    # 3. Aggregation time
    if metrics['aggregation_times']:
        ax3.plot(rounds, metrics['aggregation_times'], 'r-o', linewidth=2, markersize=8)
        ax3.set_title('Aggregation Time Per Round', fontsize=13, fontweight='bold')
        ax3.set_xlabel('Round', fontsize=11)
        ax3.set_ylabel('Time (seconds)', fontsize=11)
        ax3.grid(True, alpha=0.3)
        
        avg_agg = sum(metrics['aggregation_times']) / len(metrics['aggregation_times'])
        ax3.axhline(y=avg_agg, color='b', linestyle='--', alpha=0.5, 
                   label=f'Average: {avg_agg:.4f}s')
        ax3.legend()
    
    # 4. Evaluation time
    if metrics['evaluation_times']:
        ax4.plot(rounds, metrics['evaluation_times'], 'm-o', linewidth=2, markersize=8)
        ax4.set_title('Evaluation Time Per Round', fontsize=13, fontweight='bold')
        ax4.set_xlabel('Round', fontsize=11)
        ax4.set_ylabel('Time (seconds)', fontsize=11)
        ax4.grid(True, alpha=0.3)
        
        avg_eval = sum(metrics['evaluation_times']) / len(metrics['evaluation_times'])
        ax4.axhline(y=avg_eval, color='orange', linestyle='--', alpha=0.5, 
                   label=f'Average: {avg_eval:.2f}s')
        ax4.legend()
    
    plt.suptitle('Federated Learning Performance Metrics', 
                 fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig('performance_metrics.png', dpi=300, bbox_inches='tight')
    print("âœ“ Saved: performance_metrics.png")
    plt.show()

def print_metrics_summary(metrics):
    """Print text summary of performance metrics"""
    print("\n" + "="*60)
    print("PERFORMANCE METRICS SUMMARY")
    print("="*60)
    
    total_duration = sum(metrics['round_durations'])
    avg_duration = total_duration / len(metrics['round_durations'])
    
    total_sent = sum(metrics['bytes_sent'])
    total_received = sum(metrics['bytes_received'])
    total_bytes = total_sent + total_received
    
    print(f"\nâ±ï¸  Timing Metrics:")
    print(f"  Total Training Time: {total_duration:.2f}s ({total_duration/60:.2f} min)")
    print(f"  Average Round Duration: {avg_duration:.2f}s")
    print(f"  Minimum Round Duration: {min(metrics['round_durations']):.2f}s")
    print(f"  Maximum Round Duration: {max(metrics['round_durations']):.2f}s")
    
    if metrics['aggregation_times']:
        avg_agg = sum(metrics['aggregation_times']) / len(metrics['aggregation_times'])
        print(f"  Average Aggregation Time: {avg_agg:.4f}s")
    
    if metrics['evaluation_times']:
        avg_eval = sum(metrics['evaluation_times']) / len(metrics['evaluation_times'])
        print(f"  Average Evaluation Time: {avg_eval:.2f}s")
    
    print(f"\nðŸ“¡ Network Metrics:")
    print(f"  Total Data Sent: {total_sent/1024/1024:.2f} MB")
    print(f"  Total Data Received: {total_received/1024/1024:.2f} MB")
    print(f"  Total Network Traffic: {total_bytes/1024/1024:.2f} MB")
    print(f"  Average Traffic/Round: {(total_bytes/len(metrics['rounds']))/1024/1024:.2f} MB")
    
    print(f"\nðŸ“Š Efficiency Metrics:")
    if total_duration > 0:
        throughput = total_bytes / total_duration / 1024  # KB/s
        print(f"  Average Throughput: {throughput:.2f} KB/s")
    
    print("\n" + "="*60 + "\n")

def main():
    """Main function"""
    print("="*60)
    print("Federated Learning Performance Metrics Visualization")
    print("="*60)
    
    # Load metrics
    metrics = load_metrics()
    if metrics is None:
        return
    
    print(f"\nâœ“ Loaded performance metrics for {len(metrics['rounds'])} rounds\n")
    
    # Print summary
    print_metrics_summary(metrics)
    
    # Create visualization
    print("Creating visualization...")
    plot_metrics(metrics)
    
    print("\nâœ“ Performance metrics visualization created successfully!")

if __name__ == "__main__":
    main()
